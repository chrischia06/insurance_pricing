{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.options.display.max_columns = 100\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pol_bonus</th>\n",
       "      <th>pol_duration</th>\n",
       "      <th>pol_sit_duration</th>\n",
       "      <th>pol_payd</th>\n",
       "      <th>drv_drv2</th>\n",
       "      <th>drv_age1</th>\n",
       "      <th>drv_age_lic1</th>\n",
       "      <th>vh_age</th>\n",
       "      <th>vh_cyl</th>\n",
       "      <th>vh_din</th>\n",
       "      <th>vh_sale_begin</th>\n",
       "      <th>vh_sale_end</th>\n",
       "      <th>vh_speed</th>\n",
       "      <th>vh_value</th>\n",
       "      <th>vh_weight</th>\n",
       "      <th>town_mean_altitude</th>\n",
       "      <th>town_surface_area</th>\n",
       "      <th>population</th>\n",
       "      <th>Maxi</th>\n",
       "      <th>Median1</th>\n",
       "      <th>Median2</th>\n",
       "      <th>Mini</th>\n",
       "      <th>Professional</th>\n",
       "      <th>Retired</th>\n",
       "      <th>Biannual</th>\n",
       "      <th>Monthly</th>\n",
       "      <th>Quarterly</th>\n",
       "      <th>tourism</th>\n",
       "      <th>diesel</th>\n",
       "      <th>M</th>\n",
       "      <th>pol_bonus2</th>\n",
       "      <th>order_pol_coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>55</td>\n",
       "      <td>15</td>\n",
       "      <td>1598</td>\n",
       "      <td>111</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>185</td>\n",
       "      <td>17517</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>526.0</td>\n",
       "      <td>3216.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>33</td>\n",
       "      <td>12</td>\n",
       "      <td>2184</td>\n",
       "      <td>112</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>180</td>\n",
       "      <td>21500</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>4912.0</td>\n",
       "      <td>141.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>34</td>\n",
       "      <td>20</td>\n",
       "      <td>2496</td>\n",
       "      <td>112</td>\n",
       "      <td>32</td>\n",
       "      <td>19</td>\n",
       "      <td>130</td>\n",
       "      <td>23600</td>\n",
       "      <td>2931.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>4488.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>46</td>\n",
       "      <td>12</td>\n",
       "      <td>1149</td>\n",
       "      <td>75</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>170</td>\n",
       "      <td>13050</td>\n",
       "      <td>930.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>1339.0</td>\n",
       "      <td>61.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>35</td>\n",
       "      <td>23</td>\n",
       "      <td>1905</td>\n",
       "      <td>93</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "      <td>185</td>\n",
       "      <td>17974</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1849.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pol_bonus  pol_duration  pol_sit_duration  pol_payd  drv_drv2  drv_age1  \\\n",
       "0        0.5            36                 6         0         0        77   \n",
       "1        0.5            15                 5         0         1        52   \n",
       "2        0.5            16                 6         0         0        52   \n",
       "3        0.5            11                 7         0         0        67   \n",
       "4        0.5            16                 6         0         1        60   \n",
       "\n",
       "   drv_age_lic1  vh_age  vh_cyl  vh_din  vh_sale_begin  vh_sale_end  vh_speed  \\\n",
       "0            55      15    1598     111             16           15       185   \n",
       "1            33      12    2184     112             12            9       180   \n",
       "2            34      20    2496     112             32           19       130   \n",
       "3            46      12    1149      75             14           12       170   \n",
       "4            35      23    1905      93             23           18       185   \n",
       "\n",
       "   vh_value  vh_weight  town_mean_altitude  town_surface_area  population  \\\n",
       "0     17517     1260.0               526.0             3216.0         4.8   \n",
       "1     21500     1480.0                57.0             4912.0       141.3   \n",
       "2     23600     2931.0               257.0             4488.0         5.3   \n",
       "3     13050      930.0               109.0             1339.0        61.2   \n",
       "4     17974     1035.0                24.0             1849.0         9.7   \n",
       "\n",
       "   Maxi  Median1  Median2  Mini  Professional  Retired  Biannual  Monthly  \\\n",
       "0     1        0        0     0             0        1         1        0   \n",
       "1     1        0        0     0             0        0         0        0   \n",
       "2     0        0        0     1             1        0         0        1   \n",
       "3     1        0        0     0             0        0         1        0   \n",
       "4     0        0        1     0             0        0         1        0   \n",
       "\n",
       "   Quarterly  tourism  diesel  M  pol_bonus2  order_pol_coverage  \n",
       "0          0        1       0  1           1                   4  \n",
       "1          0        1       1  1           1                   4  \n",
       "2          0        0       1  1           1                   1  \n",
       "3          0        1       0  1           1                   4  \n",
       "4          0        1       1  1           1                   2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform(df):\n",
    "    df2 = df.copy()\n",
    "    #fill na for city data - 395 missing entries - categorical and numerical, for now assume they come from the most frequent town\n",
    "    df2['commune_code'] = df2['commune_code'].fillna(df2['commune_code'].mode()[0])\n",
    "    df2['canton_code'] = df2['canton_code'].fillna(df2['canton_code'].mode()[0])\n",
    "    df2['city_district_code'] = df2['city_district_code'].fillna(df2['city_district_code'].mode()[0])\n",
    "    df2['regional_department_code'] = df2['regional_department_code'].fillna(df2['regional_department_code'].mode()[0])\n",
    "    df2['population'] = df2['population'].fillna(df2['population'].mode()[0])\n",
    "    df2['town_mean_altitude'] = df2['town_mean_altitude'].fillna(df2['town_mean_altitude'].mode()[0])\n",
    "    df2['town_surface_area'] = df2['town_surface_area'].fillna(df2['town_surface_area'].mode()[0])\n",
    "    \n",
    "#     #impute or remove illogical values\n",
    "    df2.loc[df2['vh_weight']==0,'vh_weight'] = df2['vh_weight'].median() # - 2959 missing entries - imputation doesnt give a better correlation\n",
    "    df2 = df2.loc[df2['drv_age1']>=df2['drv_age_lic1']] # 32 missing entires; dropped\n",
    "    df2 = df2.loc[df2['vh_cyl']>0] #3 missing entries\n",
    "    df2.loc[df2['vh_value']==0,'vh_value'] = 18659 #from data exploration\n",
    "\n",
    "#     #one hot encode categorical features; \n",
    "    df2 = pd.concat([df2,pd.get_dummies(df2['pol_coverage'])],axis=1)\n",
    "    df2.loc[df2['pol_usage']=='AllTrips','pol_usage'] = 'Professional' #- only 77 AllTrips; from data description it is similar to professional\n",
    "    df2 = pd.concat([df2,pd.get_dummies(df2['pol_usage'])],axis=1)\n",
    "    df2 = pd.concat([df2,pd.get_dummies(df2['pol_pay_freq'])],axis=1)\n",
    "\n",
    "#     #binarize features\n",
    "    df2['pol_payd'] = (df2['pol_payd'] == 'Yes') * 1 # Yes/ No\n",
    "    df2['drv_drv2'] = (df2['drv_drv2'] == 'Yes') * 1 # Yes / No\n",
    "    df2['tourism'] = (df2['vh_type'] == 'Tourism') * 1 #tourism or commerical\n",
    "    df2['diesel'] = (df2['vh_fuel'] == 'Diesel') * 1 #also hybrids but very low representation - 62 hybrids\n",
    "    df2['M'] = (df2['drv_sex1'] == 'M') * 1 #Male / Female\n",
    "    \n",
    "    df2['pol_bonus2'] = (df2['pol_bonus'] == 0.5) * 1\n",
    "    \n",
    "    \n",
    "#     #ordinally encode policy coverage\n",
    "    order = {'Mini':1,'Median2':2,'Median1':3,'Maxi':4}\n",
    "    df2['order_pol_coverage'] = df['pol_coverage'].apply(lambda x : order[x])\n",
    "\n",
    "    unwantedFeatures= ['id_policy','pol_coverage','pol_pay_freq','pol_usage','pol_insee_code','drv_sex1',\n",
    "                   'drv_age2','drv_sex2','drv_age_lic2','vh_fuel','vh_make','vh_model','vh_type']+['commune_code',\n",
    "       'canton_code', 'city_district_code', 'regional_department_code']+['WorkPrivate','Yearly']+['made_claim','claim_amount']#+['drv_drv2']\n",
    "    \n",
    "    y1 = df2['made_claim']\n",
    "    y2 = df2['claim_amount']\n",
    "    df2 = df2.drop(unwantedFeatures,axis=1)\n",
    "    return y1, y2, df2\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"training_data.csv\")\n",
    "y1, y2, df2 = transform(df)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, r2_score, roc_auc_score,roc_curve\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "booleanFeatures = ['pol_payd','Professional','Retired','M','diesel','order_pol_coverage','tourism','pol_bonus2']#+['Maxi', 'Median1', 'Median2', 'Mini'] # omitted - + ['drv_drv2','vh_make','vh_model','canton_code','commune_code',city_district_code','regional_department_code','Biannual', 'Monthly','Quarterly']\n",
    "numericFeatures = ['pol_duration', 'vh_din', 'pol_bonus', 'population', 'pol_sit_duration', 'vh_value', 'vh_sale_begin', 'vh_sale_end', 'vh_cyl', 'vh_speed', 'drv_age1', 'vh_weight', 'vh_age', 'drv_age_lic1'] #+ ['order_pol_coverage','town_surface_area','town_mean_altitude']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "[x] Log Transform\n",
    "\n",
    "[x] Standard Scaling\n",
    "\n",
    "\n",
    "\n",
    "[x] PCA or not? / Kernel PCA\n",
    "\n",
    "Create Interactions / Polynomial Features\n",
    "\n",
    "Quantising numerical features?\n",
    "\n",
    "Min-Max + NMF - didnt really work\n",
    "\n",
    "LDA on boolean features - did not really work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#denoise vehicle age factors;\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "#Normalised features\n",
    "df3 = df2.copy()\n",
    "df3[numericFeatures] = ((df3[numericFeatures]-df3[numericFeatures].mean())/df3[numericFeatures].std())\n",
    "df3['order_pol_coverage']/=4\n",
    "\n",
    "#transformations\n",
    "df4 = df2.copy()\n",
    "df4['population'] = np.log(df4['population']+1)\n",
    "df4['vh_din'] = np.log(df4['vh_din'])\n",
    "df4['vh_value'] = np.log(df4['vh_value'])\n",
    "df4['vh_weight'] = np.log(df4['vh_weight'])\n",
    "df4['vh_age'] = np.log(df4['vh_age'])\n",
    "df4['vh_sale_begin'] = np.log(df4['vh_sale_begin'])\n",
    "df4[numericFeatures] = ((df4[numericFeatures]-df4[numericFeatures].mean())/df4[numericFeatures].std())\n",
    "df4['order_pol_coverage']/=4\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#PCA transformed\n",
    "pca = PCA()\n",
    "X2 = pca.fit_transform(df3[numericFeatures])[:,:12]\n",
    "X3 = pd.concat([df3[booleanFeatures].copy(),pd.DataFrame(X2,index=df3[booleanFeatures].index)],axis=1)\n",
    "\n",
    "#polynomial features\n",
    "X4 = df3[numericFeatures+booleanFeatures].copy()\n",
    "# feats = list(X4.columns)\n",
    "# for i in range(len(numericFeatures)):\n",
    "#     for j in range(i+1,len(numericFeatures+booleanFeatures)):\n",
    "#         X4[feats[i]+\"_and_\"+feats[j]] = X4[feats[i]] * X4[feats[j]]\n",
    "poly = PolynomialFeatures(degree=2,include_bias=False)\n",
    "X5 = poly.fit_transform(X4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X6 = pd.DataFrame(X5).loc[y3.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "# lr = LinearRegression()\n",
    "# lr.fit(X5,np.log(y2+1))\n",
    "# sgd = SGDRegressor()\n",
    "# sgd.fit(X5,y2)\n",
    "#0.008696886053574104 - base linear regression;\n",
    "#0.009767566771513803 - degree 2 features\n",
    "# print(r2_score(np.log(y2+1),lr.predict(X4)))\n",
    "# print(r2_score(np.log(y2+1),lr.predict(X5)))\n",
    "# plt.scatter(lr.predict(X5),np.log(y2+1))\n",
    "# plt.scatter(y2,np.exp(lr.predict(X4))-1)\n",
    "\n",
    "\n",
    "y3 = y2[y2!=0]\n",
    "lr = LinearRegression()\n",
    "# lr.fit(X6.loc[y3.index],y3)\n",
    "lr.fit(X6,y3)\n",
    "# print(r2_score(y3,lr.predict(X6.loc[y3.index])))\n",
    "print(r2_score(y3,lr.predict(pd.DataFrame(X5).loc[y3.index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit = LogisticRegression(penalty='l2',solver='saga',max_iter=500,class_weight={0:1,1:8},n_jobs=-1)\n",
    "logit.fit(X4,y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = logit.predict_proba(X4)[:,1]\n",
    "print(r2_score(y1,preds))\n",
    "print(f1_score(y1,np.round(preds)))\n",
    "print(classification_report(y1,np.round(preds)))\n",
    "confusion_matrix(y1,np.round(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dict(sorted(zip(list(X3.columns)+['intercept'],list(logit.coef_[0])+list(logit.intercept_)),key=lambda k : abs(k[1]), reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "logit = LogisticRegressionCV(cv=4,penalty='elasticnet',l1_ratios=[0.5],solver='saga',scoring='f1',max_iter=1000,class_weight={0:1,1:8},n_jobs=-1)\n",
    "logit.fit(df2[booleanFeatures+numericFeatures],y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = logit.predict_proba(df4[numericFeatures+booleanFeatures])[:,1]\n",
    "print(r2_score(y1,preds))\n",
    "print(f1_score(y1,np.round(preds)))\n",
    "print(classification_report(y1,np.round(preds)))\n",
    "confusion_matrix(y1,np.round(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Logistic Regression\n",
    "\n",
    "F1 score - 0.20665294017642946 whole dataset (logitCV); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "logit = LogisticRegressionCV(cv=4,penalty='elasticnet',l1_ratios=[0.5],solver='saga',scoring='f1',max_iter=2000,class_weight={0:1,1:8},n_jobs=-1)\n",
    "logit.fit(X3,y1)\n",
    "# print(dict(zip(list(numericFeatures)+['intercept'],list(logit.coef_[0])+list(logit.intercept_))))\n",
    "print(f1_score(y1,logit.predict(X3)))\n",
    "print(dict(sorted(zip(list(X3.columns)+['intercept'],list(logit.coef_[0])+list(logit.intercept_)),key=lambda k: abs(k[1]),reverse=True)))\n",
    "confusion_matrix(y1,logit.predict(X3))\n",
    "\n",
    "#0.22084635378639642\n",
    "# PCA - 12 components \n",
    "#booleanFeatures = ['pol_payd','Professional','Retired','M','diesel','order_pol_coverage','tourism']#+['Maxi', 'Median1', 'Median2', 'Mini'] # omitted - + ['drv_drv2','vh_make','vh_model','canton_code','commune_code',city_district_code','regional_department_code','Biannual', 'Monthly','Quarterly']\n",
    "#numericFeatures = ['pol_duration', 'vh_din', 'pol_bonus', 'population', 'pol_sit_duration', 'vh_value', 'vh_sale_begin', 'vh_sale_end', 'vh_cyl', 'vh_speed', 'drv_age1', 'vh_weight', 'vh_age', 'drv_age_lic1'] #+ ['order_pol_coverage','town_surface_area','town_mean_altitude']\n",
    "#{'intercept': -1.2816726233158604, 'order_pol_coverage': 0.8818957942043697, 'Professional': 0.28509350856548965, 'pol_payd': -0.199076304952475, 0: 0.1853148764034734, 'diesel': 0.17271695034768617, 10: 0.1709972763750221, 1: -0.11689737479236778, 11: 0.10776916230010722, 'tourism': 0.09403280941367201, 5: 0.0894471102207708, 'Retired': 0.08741246835566338, 9: -0.08088024876285624, 2: -0.07405054993731437, 3: 0.06053853062186241, 'M': -0.04357310622682324, 7: 0.03256260809907931, 8: -0.006274962263683822, 4: 0.005540399001789366, 6: 0.002016743325228757}\n",
    "#logit = LogisticRegressionCV(cv=4,penalty='l2',solver='saga',scoring='f1',max_iter=1000,class_weight={0:1,1:9.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "logit = LogisticRegressionCV(cv=4,penalty='elasticnet',l1_ratios=[0.5],solver='saga',scoring='f1',max_iter=2000,class_weight={0:1,1:8},n_jobs=-1)\n",
    "logit.fit(df3[numericFeatures+booleanFeatures],y1)\n",
    "# print(dict(zip(list(numericFeatures)+['intercept'],list(logit.coef_[0])+list(logit.intercept_))))\n",
    "print(f1_score(y1,logit.predict(df3[numericFeatures+booleanFeatures])))\n",
    "print(dict(sorted(zip(list(df3[numericFeatures+booleanFeatures].columns)+['intercept'],list(logit.coef_[0])+list(logit.intercept_)),key=lambda k: abs(k[1]),reverse=True)))\n",
    "confusion_matrix(y1,logit.predict(df3[numericFeatures+booleanFeatures]))\n",
    "\n",
    "#0.22043443282381336 - No PCA{'pol_payd': -1.2824996176580108, 1: -1.2809208687711864, 2: -1.2502802183077386, 9: -1.2106389710837047, 'M': -1.2028787104330692, 'drv_drv2': -1.1717891919876045, 'tourism': -1.1717891919876045, 6: -1.1717891919876045, 8: -1.1717891919876045, 12: -1.1717891919876045, 13: -1.1717891919876045, 4: -1.1690840663049051, 'Retired': -1.1513352412308835, 7: -1.139096142954687, 11: -1.1181827981955113, 3: -1.109005694367409, 5: -1.0805319282957322, 10: -1.0399889096109443, 'diesel': -0.9989086254790716, 0: -0.9861306618692427, 'order_pol_coverage': -0.9488964084112984}\n",
    "#booleanFeatures = ['pol_payd','Professional','Retired','M','diesel','order_pol_coverage','tourism']#+['Maxi', 'Median1', 'Median2', 'Mini'] # omitted - + ['drv_drv2','vh_make','vh_model','canton_code','commune_code',city_district_code','regional_department_code','Biannual', 'Monthly','Quarterly']\n",
    "#numericFeatures = ['pol_duration', 'vh_din', 'pol_bonus', 'population', 'pol_sit_duration', 'vh_value', 'vh_sale_begin', 'vh_sale_end', 'vh_cyl', 'vh_speed', 'drv_age1', 'vh_weight', 'vh_age', 'drv_age_lic1'] #+ ['order_pol_coverage','town_surface_area','town_mean_altitude']\n",
    "\n",
    "#logit = LogisticRegressionCV(cv=4,penalty='elasticnet',l1_ratios=[0.5],solver='saga',scoring='f1',max_iter=2000,class_weight={0:1,1:8},n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "logit = LogisticRegressionCV(cv=2,penalty='l2',solver='saga',scoring='f1',max_iter=3000,class_weight={0:1,1:8},n_jobs=-1)\n",
    "logit.fit(df4[numericFeatures+booleanFeatures],y1)\n",
    "# print(dict(zip(list(numericFeatures)+['intercept'],list(logit.coef_[0])+list(logit.intercept_))))\n",
    "\n",
    "preds = logit.predict(df4[numericFeatures+booleanFeatures])\n",
    "print(f1_score(y1,preds))\n",
    "print(dict(sorted(zip(list(df4[numericFeatures+booleanFeatures].columns)+['intercept'],list(logit.coef_[0])+list(logit.intercept_)),key=lambda k: abs(k[1]),reverse=True)),\"\\n\")\n",
    "print(classification_report(y1,preds))\n",
    "confusion_matrix(y1,preds)\n",
    "\n",
    "#0.22043443282381336 - No PCA{'pol_payd': -1.2824996176580108, 1: -1.2809208687711864, 2: -1.2502802183077386, 9: -1.2106389710837047, 'M': -1.2028787104330692, 'drv_drv2': -1.1717891919876045, 'tourism': -1.1717891919876045, 6: -1.1717891919876045, 8: -1.1717891919876045, 12: -1.1717891919876045, 13: -1.1717891919876045, 4: -1.1690840663049051, 'Retired': -1.1513352412308835, 7: -1.139096142954687, 11: -1.1181827981955113, 3: -1.109005694367409, 5: -1.0805319282957322, 10: -1.0399889096109443, 'diesel': -0.9989086254790716, 0: -0.9861306618692427, 'order_pol_coverage': -0.9488964084112984}\n",
    "#booleanFeatures = ['pol_payd','Professional','Retired','M','diesel','order_pol_coverage','tourism']#+['Maxi', 'Median1', 'Median2', 'Mini'] # omitted - + ['drv_drv2','vh_make','vh_model','canton_code','commune_code',city_district_code','regional_department_code','Biannual', 'Monthly','Quarterly']\n",
    "#numericFeatures = ['pol_duration', 'vh_din', 'pol_bonus', 'population', 'pol_sit_duration', 'vh_value', 'vh_sale_begin', 'vh_sale_end', 'vh_cyl', 'vh_speed', 'drv_age1', 'vh_weight', 'vh_age', 'drv_age_lic1'] #+ ['order_pol_coverage','town_surface_area','town_mean_altitude']\n",
    "\n",
    "#logit = LogisticRegressionCV(cv=4,penalty='elasticnet',l1_ratios=[0.5],solver='saga',scoring='f1',max_iter=2000,class_weight={0:1,1:8},n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3['preds'] = logit.predict_proba(X3)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "((X3['preds']>0.5)*50+100).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df3[numericFeatures+booleanFeatures+['preds']].loc[y3.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4 = pd.DataFrame(X4,index=X3.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, ElasticNet,Lasso, BayesianRidge\n",
    "\n",
    "y3 = y2[y2!=0]\n",
    "lr = LinearRegression()\n",
    "# lr.fit(X6.loc[y3.index],y3)\n",
    "lr.fit(X6,np.log(y2+1))\n",
    "# print(r2_score(y3,lr.predict(X6.loc[y3.index])))\n",
    "print(r2_score(np.log(y2+1),lr.predict(X6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X6['severity'] = lr.predict(X6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X6['severity'].corr(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = y2[y2!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "gpr = GaussianProcessRegressor(copy_X_train=False)\n",
    "gpr.fit(X3.loc[y3.index],np.log(y3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gpr.predict(X3.loc[y3.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(r2_score(y2.iloc[:1000],np.exp(gpr.predict(X3.iloc[:1000]))))\n",
    "plt.scatter(np.exp(gpr.predict(X3.iloc[:1000])),y2.iloc[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "1. Use Logistic Regression to predict probability, then use Linear Regression to predict severity including the probability\n",
    "    1. Use Linear Regression to predict severity, then use Logistic Regression to predict probability including predicted severity\n",
    "2. Same but Use Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Classifiers\n",
    "\n",
    "[x] Logistic Regression  - 0.2 F1 so far\n",
    "\n",
    "[x] Naive bayes - some success using ComplementNB\n",
    "\n",
    "[x] GradientBoosting \n",
    "\n",
    "[x] Random Forest, ExtraTrees - similar performance\n",
    "\n",
    "[x] XGBOOST\n",
    "\n",
    "[x] SVC \n",
    "\n",
    "[x] KNeighbors - worked on numericFeatures - too large for submission\n",
    "\n",
    "Gaussian Mixture - Data not gaussian\n",
    "\n",
    "Gaussian Process - too slow\n",
    "\n",
    "SGD, Perceptron - didnt really work\n",
    "\n",
    "HistGradientBoosting - did not work\n",
    "\n",
    "BaggingClassifier, Adaboost - slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Metrics\n",
    "Accuracy - fraction of correct predictions - causes to predict default 0\n",
    "\n",
    "Balanced Accuracy\n",
    "\n",
    "Precision - ability to not have false negatives\n",
    "Recall - Ability to not find true positives\n",
    "\n",
    "*F1 score* - weighted measure of precision and recall\n",
    "\n",
    "ROC curve -  performance of a binary classifier system as its discrimination threshold is varied\n",
    "\n",
    "roc_auc_curve score\n",
    "\n",
    "brier_score_loss\n",
    "\n",
    "hinge-loss\n",
    "\n",
    "roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "PCA\n",
    "\n",
    "Polynomial Features / Interactions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
