{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.options.display.max_columns = 100\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pol_bonus</th>\n",
       "      <th>pol_duration</th>\n",
       "      <th>pol_sit_duration</th>\n",
       "      <th>pol_payd</th>\n",
       "      <th>drv_drv2</th>\n",
       "      <th>drv_age1</th>\n",
       "      <th>drv_age_lic1</th>\n",
       "      <th>vh_age</th>\n",
       "      <th>vh_cyl</th>\n",
       "      <th>vh_din</th>\n",
       "      <th>vh_sale_begin</th>\n",
       "      <th>vh_sale_end</th>\n",
       "      <th>vh_speed</th>\n",
       "      <th>vh_value</th>\n",
       "      <th>vh_weight</th>\n",
       "      <th>town_mean_altitude</th>\n",
       "      <th>town_surface_area</th>\n",
       "      <th>population</th>\n",
       "      <th>Maxi</th>\n",
       "      <th>Median1</th>\n",
       "      <th>Median2</th>\n",
       "      <th>Mini</th>\n",
       "      <th>Professional</th>\n",
       "      <th>Retired</th>\n",
       "      <th>Biannual</th>\n",
       "      <th>Monthly</th>\n",
       "      <th>Quarterly</th>\n",
       "      <th>tourism</th>\n",
       "      <th>diesel</th>\n",
       "      <th>M</th>\n",
       "      <th>pol_bonus2</th>\n",
       "      <th>order_pol_coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>55</td>\n",
       "      <td>15</td>\n",
       "      <td>1598</td>\n",
       "      <td>111</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>185</td>\n",
       "      <td>17517</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>526.0</td>\n",
       "      <td>3216.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>33</td>\n",
       "      <td>12</td>\n",
       "      <td>2184</td>\n",
       "      <td>112</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>180</td>\n",
       "      <td>21500</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>4912.0</td>\n",
       "      <td>141.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>34</td>\n",
       "      <td>20</td>\n",
       "      <td>2496</td>\n",
       "      <td>112</td>\n",
       "      <td>32</td>\n",
       "      <td>19</td>\n",
       "      <td>130</td>\n",
       "      <td>23600</td>\n",
       "      <td>2931.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>4488.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>46</td>\n",
       "      <td>12</td>\n",
       "      <td>1149</td>\n",
       "      <td>75</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>170</td>\n",
       "      <td>13050</td>\n",
       "      <td>930.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>1339.0</td>\n",
       "      <td>61.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>35</td>\n",
       "      <td>23</td>\n",
       "      <td>1905</td>\n",
       "      <td>93</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "      <td>185</td>\n",
       "      <td>17974</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1849.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pol_bonus  pol_duration  pol_sit_duration  pol_payd  drv_drv2  drv_age1  \\\n",
       "0        0.5            36                 6         0         0        77   \n",
       "1        0.5            15                 5         0         1        52   \n",
       "2        0.5            16                 6         0         0        52   \n",
       "3        0.5            11                 7         0         0        67   \n",
       "4        0.5            16                 6         0         1        60   \n",
       "\n",
       "   drv_age_lic1  vh_age  vh_cyl  vh_din  vh_sale_begin  vh_sale_end  vh_speed  \\\n",
       "0            55      15    1598     111             16           15       185   \n",
       "1            33      12    2184     112             12            9       180   \n",
       "2            34      20    2496     112             32           19       130   \n",
       "3            46      12    1149      75             14           12       170   \n",
       "4            35      23    1905      93             23           18       185   \n",
       "\n",
       "   vh_value  vh_weight  town_mean_altitude  town_surface_area  population  \\\n",
       "0     17517     1260.0               526.0             3216.0         4.8   \n",
       "1     21500     1480.0                57.0             4912.0       141.3   \n",
       "2     23600     2931.0               257.0             4488.0         5.3   \n",
       "3     13050      930.0               109.0             1339.0        61.2   \n",
       "4     17974     1035.0                24.0             1849.0         9.7   \n",
       "\n",
       "   Maxi  Median1  Median2  Mini  Professional  Retired  Biannual  Monthly  \\\n",
       "0     1        0        0     0             0        1         1        0   \n",
       "1     1        0        0     0             0        0         0        0   \n",
       "2     0        0        0     1             1        0         0        1   \n",
       "3     1        0        0     0             0        0         1        0   \n",
       "4     0        0        1     0             0        0         1        0   \n",
       "\n",
       "   Quarterly  tourism  diesel  M  pol_bonus2  order_pol_coverage  \n",
       "0          0        1       0  1           1                   4  \n",
       "1          0        1       1  1           1                   4  \n",
       "2          0        0       1  1           1                   1  \n",
       "3          0        1       0  1           1                   4  \n",
       "4          0        1       1  1           1                   2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform(df):\n",
    "    df2 = df.copy()\n",
    "    #fill na for city data - 395 missing entries - categorical and numerical, for now assume they come from the most frequent town\n",
    "    df2['commune_code'] = df2['commune_code'].fillna(df2['commune_code'].mode()[0])\n",
    "    df2['canton_code'] = df2['canton_code'].fillna(df2['canton_code'].mode()[0])\n",
    "    df2['city_district_code'] = df2['city_district_code'].fillna(df2['city_district_code'].mode()[0])\n",
    "    df2['regional_department_code'] = df2['regional_department_code'].fillna(df2['regional_department_code'].mode()[0])\n",
    "    df2['population'] = df2['population'].fillna(df2['population'].mode()[0])\n",
    "    df2['town_mean_altitude'] = df2['town_mean_altitude'].fillna(df2['town_mean_altitude'].mode()[0])\n",
    "    df2['town_surface_area'] = df2['town_surface_area'].fillna(df2['town_surface_area'].mode()[0])\n",
    "    \n",
    "#     #impute or remove illogical values\n",
    "    df2.loc[df2['vh_weight']==0,'vh_weight'] = df2['vh_weight'].median() # - 2959 missing entries - imputation doesnt give a better correlation\n",
    "    df2 = df2.loc[df2['drv_age1']>=df2['drv_age_lic1']] # 32 missing entires; dropped\n",
    "    df2 = df2.loc[df2['vh_cyl']>0] #3 missing entries\n",
    "    df2.loc[df2['vh_value']==0,'vh_value'] = 18659 #from data exploration\n",
    "\n",
    "#     #one hot encode categorical features; \n",
    "    df2 = pd.concat([df2,pd.get_dummies(df2['pol_coverage'])],axis=1)\n",
    "    df2.loc[df2['pol_usage']=='AllTrips','pol_usage'] = 'Professional' #- only 77 AllTrips; from data description it is similar to professional\n",
    "    df2 = pd.concat([df2,pd.get_dummies(df2['pol_usage'])],axis=1)\n",
    "    df2 = pd.concat([df2,pd.get_dummies(df2['pol_pay_freq'])],axis=1)\n",
    "\n",
    "#     #binarize features\n",
    "    df2['pol_payd'] = (df2['pol_payd'] == 'Yes') * 1 # Yes/ No\n",
    "    df2['drv_drv2'] = (df2['drv_drv2'] == 'Yes') * 1 # Yes / No\n",
    "    df2['tourism'] = (df2['vh_type'] == 'Tourism') * 1 #tourism or commerical\n",
    "    df2['diesel'] = (df2['vh_fuel'] == 'Diesel') * 1 #also hybrids but very low representation - 62 hybrids\n",
    "    df2['M'] = (df2['drv_sex1'] == 'M') * 1 #Male / Female\n",
    "    \n",
    "    df2['pol_bonus2'] = (df2['pol_bonus'] == 0.5) * 1\n",
    "    \n",
    "    \n",
    "#     #ordinally encode policy coverage\n",
    "    order = {'Mini':1,'Median2':2,'Median1':3,'Maxi':4}\n",
    "    df2['order_pol_coverage'] = df['pol_coverage'].apply(lambda x : order[x])\n",
    "\n",
    "    unwantedFeatures= ['id_policy','pol_coverage','pol_pay_freq','pol_usage','pol_insee_code','drv_sex1',\n",
    "                   'drv_age2','drv_sex2','drv_age_lic2','vh_fuel','vh_make','vh_model','vh_type']+['commune_code',\n",
    "       'canton_code', 'city_district_code', 'regional_department_code']+['WorkPrivate','Yearly']+['made_claim','claim_amount']#+['drv_drv2']\n",
    "    \n",
    "    y1 = df2['made_claim']\n",
    "    y2 = df2['claim_amount']\n",
    "    df2 = df2.drop(unwantedFeatures,axis=1)\n",
    "    return y1, y2, df2\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"training_data.csv\")\n",
    "y1, y2, df2 = transform(df)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, r2_score, roc_auc_score,roc_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "booleanFeatures = ['pol_payd','Professional','Retired','M','diesel','order_pol_coverage','tourism','pol_bonus2']#+['Maxi', 'Median1', 'Median2', 'Mini'] # omitted - + ['drv_drv2','vh_make','vh_model','canton_code','commune_code',city_district_code','regional_department_code','Biannual', 'Monthly','Quarterly']\n",
    "numericFeatures = ['pol_duration', 'vh_din', 'pol_bonus', 'population', 'pol_sit_duration', 'vh_value', 'vh_sale_begin', 'vh_sale_end', 'vh_cyl', 'vh_speed', 'drv_age1', 'vh_weight', 'vh_age', 'drv_age_lic1'] #+ ['order_pol_coverage','town_surface_area','town_mean_altitude']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "[x] Log Transform\n",
    "\n",
    "[x] Standard Scaling\n",
    "\n",
    "\n",
    "\n",
    "[x] PCA or not? / Kernel PCA\n",
    "\n",
    "Create Interactions / Polynomial Features\n",
    "\n",
    "Quantising numerical features?\n",
    "\n",
    "Min-Max + NMF - didnt really work\n",
    "\n",
    "LDA on boolean features - did not really work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#denoise vehicle age factors;\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "#Normalised features\n",
    "df3 = df2.copy()\n",
    "df3[numericFeatures] = ((df3[numericFeatures]-df3[numericFeatures].mean())/df3[numericFeatures].std())\n",
    "df3['order_pol_coverage']/=4\n",
    "\n",
    "#transformations\n",
    "df4 = df2.copy()\n",
    "df4['population'] = np.log(df4['population']+1)\n",
    "df4['vh_din'] = np.log(df4['vh_din'])\n",
    "df4['vh_value'] = np.log(df4['vh_value'])\n",
    "df4['vh_weight'] = np.log(df4['vh_weight'])\n",
    "df4['vh_age'] = np.log(df4['vh_age'])\n",
    "df4['vh_sale_begin'] = np.log(df4['vh_sale_begin'])\n",
    "df4[numericFeatures] = ((df4[numericFeatures]-df4[numericFeatures].mean())/df4[numericFeatures].std())\n",
    "df4['order_pol_coverage']/=4\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#PCA transformed\n",
    "pca = PCA()\n",
    "X2 = pca.fit_transform(df3[numericFeatures])[:,:12]\n",
    "X3 = pd.concat([df3[booleanFeatures].copy(),pd.DataFrame(X2,index=df3[booleanFeatures].index)],axis=1)\n",
    "\n",
    "#polynomial features\n",
    "X4 = df3[numericFeatures+booleanFeatures].copy()\n",
    "# feats = list(X4.columns)\n",
    "# for i in range(len(numericFeatures)):\n",
    "#     for j in range(i+1,len(numericFeatures+booleanFeatures)):\n",
    "#         X4[feats[i]+\"_and_\"+feats[j]] = X4[feats[i]] * X4[feats[j]]\n",
    "poly = PolynomialFeatures(degree=2,include_bias=False)\n",
    "X5 = poly.fit_transform(X4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X6 = pd.DataFrame(X5).loc[y3.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "# lr = LinearRegression()\n",
    "# lr.fit(X5,np.log(y2+1))\n",
    "# sgd = SGDRegressor()\n",
    "# sgd.fit(X5,y2)\n",
    "#0.008696886053574104 - base linear regression;\n",
    "#0.009767566771513803 - degree 2 features\n",
    "# print(r2_score(np.log(y2+1),lr.predict(X4)))\n",
    "# print(r2_score(np.log(y2+1),lr.predict(X5)))\n",
    "# plt.scatter(lr.predict(X5),np.log(y2+1))\n",
    "# plt.scatter(y2,np.exp(lr.predict(X4))-1)\n",
    "\n",
    "\n",
    "y3 = y2[y2!=0]\n",
    "lr = LinearRegression()\n",
    "# lr.fit(X6.loc[y3.index],y3)\n",
    "lr.fit(X6,y3)\n",
    "# print(r2_score(y3,lr.predict(X6.loc[y3.index])))\n",
    "print(r2_score(y3,lr.predict(pd.DataFrame(X5).loc[y3.index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit = LogisticRegression(penalty='l2',solver='saga',max_iter=500,class_weight={0:1,1:8},n_jobs=-1)\n",
    "logit.fit(X4,y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = logit.predict_proba(X4)[:,1]\n",
    "print(r2_score(y1,preds))\n",
    "print(f1_score(y1,np.round(preds)))\n",
    "print(classification_report(y1,np.round(preds)))\n",
    "confusion_matrix(y1,np.round(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dict(sorted(zip(list(X3.columns)+['intercept'],list(logit.coef_[0])+list(logit.intercept_)),key=lambda k : abs(k[1]), reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "logit = LogisticRegressionCV(cv=4,penalty='elasticnet',l1_ratios=[0.5],solver='saga',scoring='f1',max_iter=1000,class_weight={0:1,1:8},n_jobs=-1)\n",
    "logit.fit(df2[booleanFeatures+numericFeatures],y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = logit.predict_proba(df4[numericFeatures+booleanFeatures])[:,1]\n",
    "print(r2_score(y1,preds))\n",
    "print(f1_score(y1,np.round(preds)))\n",
    "print(classification_report(y1,np.round(preds)))\n",
    "confusion_matrix(y1,np.round(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Logistic Regression\n",
    "\n",
    "F1 score - 0.20665294017642946 whole dataset (logitCV); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "logit = LogisticRegressionCV(cv=4,penalty='elasticnet',l1_ratios=[0.5],solver='saga',scoring='f1',max_iter=2000,class_weight={0:1,1:8},n_jobs=-1)\n",
    "logit.fit(X3,y1)\n",
    "# print(dict(zip(list(numericFeatures)+['intercept'],list(logit.coef_[0])+list(logit.intercept_))))\n",
    "print(f1_score(y1,logit.predict(X3)))\n",
    "print(dict(sorted(zip(list(X3.columns)+['intercept'],list(logit.coef_[0])+list(logit.intercept_)),key=lambda k: abs(k[1]),reverse=True)))\n",
    "confusion_matrix(y1,logit.predict(X3))\n",
    "\n",
    "#0.22084635378639642\n",
    "# PCA - 12 components \n",
    "#booleanFeatures = ['pol_payd','Professional','Retired','M','diesel','order_pol_coverage','tourism']#+['Maxi', 'Median1', 'Median2', 'Mini'] # omitted - + ['drv_drv2','vh_make','vh_model','canton_code','commune_code',city_district_code','regional_department_code','Biannual', 'Monthly','Quarterly']\n",
    "#numericFeatures = ['pol_duration', 'vh_din', 'pol_bonus', 'population', 'pol_sit_duration', 'vh_value', 'vh_sale_begin', 'vh_sale_end', 'vh_cyl', 'vh_speed', 'drv_age1', 'vh_weight', 'vh_age', 'drv_age_lic1'] #+ ['order_pol_coverage','town_surface_area','town_mean_altitude']\n",
    "#{'intercept': -1.2816726233158604, 'order_pol_coverage': 0.8818957942043697, 'Professional': 0.28509350856548965, 'pol_payd': -0.199076304952475, 0: 0.1853148764034734, 'diesel': 0.17271695034768617, 10: 0.1709972763750221, 1: -0.11689737479236778, 11: 0.10776916230010722, 'tourism': 0.09403280941367201, 5: 0.0894471102207708, 'Retired': 0.08741246835566338, 9: -0.08088024876285624, 2: -0.07405054993731437, 3: 0.06053853062186241, 'M': -0.04357310622682324, 7: 0.03256260809907931, 8: -0.006274962263683822, 4: 0.005540399001789366, 6: 0.002016743325228757}\n",
    "#logit = LogisticRegressionCV(cv=4,penalty='l2',solver='saga',scoring='f1',max_iter=1000,class_weight={0:1,1:9.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "logit = LogisticRegressionCV(cv=4,penalty='elasticnet',l1_ratios=[0.5],solver='saga',scoring='f1',max_iter=2000,class_weight={0:1,1:8},n_jobs=-1)\n",
    "logit.fit(df3[numericFeatures+booleanFeatures],y1)\n",
    "# print(dict(zip(list(numericFeatures)+['intercept'],list(logit.coef_[0])+list(logit.intercept_))))\n",
    "print(f1_score(y1,logit.predict(df3[numericFeatures+booleanFeatures])))\n",
    "print(dict(sorted(zip(list(df3[numericFeatures+booleanFeatures].columns)+['intercept'],list(logit.coef_[0])+list(logit.intercept_)),key=lambda k: abs(k[1]),reverse=True)))\n",
    "confusion_matrix(y1,logit.predict(df3[numericFeatures+booleanFeatures]))\n",
    "\n",
    "#0.22043443282381336 - No PCA{'pol_payd': -1.2824996176580108, 1: -1.2809208687711864, 2: -1.2502802183077386, 9: -1.2106389710837047, 'M': -1.2028787104330692, 'drv_drv2': -1.1717891919876045, 'tourism': -1.1717891919876045, 6: -1.1717891919876045, 8: -1.1717891919876045, 12: -1.1717891919876045, 13: -1.1717891919876045, 4: -1.1690840663049051, 'Retired': -1.1513352412308835, 7: -1.139096142954687, 11: -1.1181827981955113, 3: -1.109005694367409, 5: -1.0805319282957322, 10: -1.0399889096109443, 'diesel': -0.9989086254790716, 0: -0.9861306618692427, 'order_pol_coverage': -0.9488964084112984}\n",
    "#booleanFeatures = ['pol_payd','Professional','Retired','M','diesel','order_pol_coverage','tourism']#+['Maxi', 'Median1', 'Median2', 'Mini'] # omitted - + ['drv_drv2','vh_make','vh_model','canton_code','commune_code',city_district_code','regional_department_code','Biannual', 'Monthly','Quarterly']\n",
    "#numericFeatures = ['pol_duration', 'vh_din', 'pol_bonus', 'population', 'pol_sit_duration', 'vh_value', 'vh_sale_begin', 'vh_sale_end', 'vh_cyl', 'vh_speed', 'drv_age1', 'vh_weight', 'vh_age', 'drv_age_lic1'] #+ ['order_pol_coverage','town_surface_area','town_mean_altitude']\n",
    "\n",
    "#logit = LogisticRegressionCV(cv=4,penalty='elasticnet',l1_ratios=[0.5],solver='saga',scoring='f1',max_iter=2000,class_weight={0:1,1:8},n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "logit = LogisticRegressionCV(cv=2,penalty='l2',solver='saga',scoring='f1',max_iter=3000,class_weight={0:1,1:8},n_jobs=-1)\n",
    "logit.fit(df4[numericFeatures+booleanFeatures],y1)\n",
    "# print(dict(zip(list(numericFeatures)+['intercept'],list(logit.coef_[0])+list(logit.intercept_))))\n",
    "\n",
    "preds = logit.predict(df4[numericFeatures+booleanFeatures])\n",
    "print(f1_score(y1,preds))\n",
    "print(dict(sorted(zip(list(df4[numericFeatures+booleanFeatures].columns)+['intercept'],list(logit.coef_[0])+list(logit.intercept_)),key=lambda k: abs(k[1]),reverse=True)),\"\\n\")\n",
    "print(classification_report(y1,preds))\n",
    "confusion_matrix(y1,preds)\n",
    "\n",
    "#0.22043443282381336 - No PCA{'pol_payd': -1.2824996176580108, 1: -1.2809208687711864, 2: -1.2502802183077386, 9: -1.2106389710837047, 'M': -1.2028787104330692, 'drv_drv2': -1.1717891919876045, 'tourism': -1.1717891919876045, 6: -1.1717891919876045, 8: -1.1717891919876045, 12: -1.1717891919876045, 13: -1.1717891919876045, 4: -1.1690840663049051, 'Retired': -1.1513352412308835, 7: -1.139096142954687, 11: -1.1181827981955113, 3: -1.109005694367409, 5: -1.0805319282957322, 10: -1.0399889096109443, 'diesel': -0.9989086254790716, 0: -0.9861306618692427, 'order_pol_coverage': -0.9488964084112984}\n",
    "#booleanFeatures = ['pol_payd','Professional','Retired','M','diesel','order_pol_coverage','tourism']#+['Maxi', 'Median1', 'Median2', 'Mini'] # omitted - + ['drv_drv2','vh_make','vh_model','canton_code','commune_code',city_district_code','regional_department_code','Biannual', 'Monthly','Quarterly']\n",
    "#numericFeatures = ['pol_duration', 'vh_din', 'pol_bonus', 'population', 'pol_sit_duration', 'vh_value', 'vh_sale_begin', 'vh_sale_end', 'vh_cyl', 'vh_speed', 'drv_age1', 'vh_weight', 'vh_age', 'drv_age_lic1'] #+ ['order_pol_coverage','town_surface_area','town_mean_altitude']\n",
    "\n",
    "#logit = LogisticRegressionCV(cv=4,penalty='elasticnet',l1_ratios=[0.5],solver='saga',scoring='f1',max_iter=2000,class_weight={0:1,1:8},n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3['preds'] = logit.predict_proba(X3)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "((X3['preds']>0.5)*50+100).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df3[numericFeatures+booleanFeatures+['preds']].loc[y3.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4 = pd.DataFrame(X4,index=X3.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, ElasticNet,Lasso, BayesianRidge\n",
    "\n",
    "y3 = y2[y2!=0]\n",
    "lr = LinearRegression()\n",
    "# lr.fit(X6.loc[y3.index],y3)\n",
    "lr.fit(X6,np.log(y2+1))\n",
    "# print(r2_score(y3,lr.predict(X6.loc[y3.index])))\n",
    "print(r2_score(np.log(y2+1),lr.predict(X6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X6['severity'] = lr.predict(X6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X6['severity'].corr(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = y2[y2!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "gpr = GaussianProcessRegressor(copy_X_train=False)\n",
    "gpr.fit(X3.loc[y3.index],np.log(y3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gpr.predict(X3.loc[y3.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(r2_score(y2.iloc[:1000],np.exp(gpr.predict(X3.iloc[:1000]))))\n",
    "plt.scatter(np.exp(gpr.predict(X3.iloc[:1000])),y2.iloc[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "logit = LogisticRegressionCV(cv=2,penalty='l2',solver='saga',scoring='f1',max_iter=1000,class_weight={0:1,1:9.1},n_jobs=-1)\n",
    "clf = CalibratedClassifierCV(logit,cv=2,method='sigmoid')\n",
    "clf.fit(df4,y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods - Gradient Boosting, Random Forest, XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.737667974674193\n",
      "0.428299794010458\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.83      0.89     72125\n",
      "           1       0.30      0.74      0.43      7260\n",
      "\n",
      "    accuracy                           0.82     79385\n",
      "   macro avg       0.64      0.79      0.66     79385\n",
      "weighted avg       0.91      0.82      0.85     79385\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[59547, 12578],\n",
       "       [ 1854,  5406]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier,ExtraTreesClassifier\n",
    "sample_weights = np.zeros(len(y1))\n",
    "sample_weights[y1 == 0] = 1\n",
    "sample_weights[y1 == 1] = 8\n",
    "\n",
    "gbc = GradientBoostingClassifier(learning_rate=0.1,n_estimators=500,max_depth=5)\n",
    "gbc.fit(X4,y1,sample_weight=sample_weights)\n",
    "\n",
    "preds = gbc.predict_proba(X4)[:,1]\n",
    "print(r2_score(y1,preds))\n",
    "print(f1_score(y1,np.round(preds)))\n",
    "print(classification_report(y1,np.round(preds)))\n",
    "confusion_matrix(y1,np.round(preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(gbc,X4,y1,cv=3,scoring='f1')\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(20,10))\n",
    "rfFeatImportances = dict(sorted(zip(numericFeatures+booleanFeatures+['preds'],gbr.feature_importances_),key=lambda k: k[1]))\n",
    "plt.barh(list(rfFeatImportances.keys()),rfFeatImportances.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "fig,ax = plt.subplots(figsize=(20,10))\n",
    "plot_tree(gbc.estimators_[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "lr = LinearRegression()\n",
    "gbr = GradientBoostingRegressor(n_estimators=300,max_depth=3,loss='ls',init=lr)\n",
    "gbr.fit(df3[numericFeatures+booleanFeatures],y2)\n",
    "preds = gbr.predict(X4)\n",
    "print(\"R2 score\",r2_score(y2,preds))\n",
    "plt.scatter(preds,y2)\n",
    "#R2 score 0.5782434142409241\n",
    "#gbr = GradientBoostingRegressor(n_estimators=300,max_depth=3,loss='ls',init=lr)\n",
    "#numericFeatures+booleanFeatures+['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X4,y2)\n",
    "preds = lr.predict(X4)\n",
    "print(\"R2 score\",r2_score(y2,preds))\n",
    "plt.scatter(preds,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier,BaggingClassifier,AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc = ExtraTreesClassifier(n_estimators=100,max_depth=10,class_weight={0:1,1:9})\n",
    "\n",
    "scores = cross_val_score(hgbc,df3[numericFeatures+booleanFeatures],y1,cv=3,scoring='f1')\n",
    "print(scores.mean())\n",
    "hgbc.fit(df3[numericFeatures+booleanFeatures],y1)\n",
    "preds = hgbc.predict(df3[numericFeatures+booleanFeatures])\n",
    "print(\"F1 score\",f1_score(y1,preds))\n",
    "confusion_matrix(y1,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(20,10))\n",
    "rfFeatImportances = dict(sorted(zip(numericFeatures+booleanFeatures,hgbc.feature_importances_),key=lambda k: k[1]))\n",
    "plt.barh(list(rfFeatImportances.keys()),rfFeatImportances.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "xbgc = XGBClassifier(base_score=0.9,max_depth=8)\n",
    "\n",
    "xbgc.fit(df2[numericFeatures+booleanFeatures],y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xbgc.predict(df2[numericFeatures+booleanFeatures])\n",
    "# print(f1_score(y1,preds))\n",
    "# confusion_matrix(y1,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfFeatures = list(df3.columns)\n",
    "# rfFeatures = rfFeatures2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "rf = RandomForestClassifier(n_estimators=500,\n",
    "                            class_weight={0:0.1,1:0.9},\n",
    "                            max_depth=8,\n",
    "                           max_features='auto',\n",
    "                           min_samples_split=2,\n",
    "                           min_samples_leaf=1)\n",
    "# rfFeatures = ['town_mean_altitude', 'Retired', 'town_surface_area', 'tourism', 'WorkPrivate', 'M', 'pol_duration', 'Median1', 'Professional', 'population', 'Median2', 'pol_bonus', 'diesel', 'vh_cyl', 'vh_weight', 'vh_speed', 'vh_din', 'vh_value', 'Mini', 'Maxi', 'order_pol_coverage', 'vh_age', 'vh_sale_end', 'vh_sale_begin']\n",
    "\n",
    "scores = cross_val_score(rf,df3[numericFeatures+booleanFeatures],y1,scoring='f1',cv=4)\n",
    "print(scores.mean())\n",
    "#0.21783976920072673 - All Features - except pol_payd,drv_drv2,  n_estimators=300,class_weight={0:0.1,1:0.9},max_depth=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#F1 score 0.24166413296848077, 4769 true positives 27439 false positives\n",
    "rf.fit(df4[rfFeatures],y1)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "preds = rf.predict(df3[rfFeatures])\n",
    "print(\"F1 score\",f1_score(y1,preds))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y1,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(20,10))\n",
    "rfFeatImportances = dict(sorted(zip(rfFeatures,rf.feature_importances_),key=lambda k: k[1]))\n",
    "plt.barh(list(rfFeatImportances.keys()),rfFeatImportances.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB,ComplementNB,CategoricalNB,GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "bnb = ComplementNB(class_prior=[0.5,0.5])\n",
    "# bnb = GaussianNB(priors=[0.8,0.2])\n",
    "\n",
    "booleanFeatures2 = ['pol_payd','Maxi', 'Median1', 'Median2', 'Mini','tourism','drv_drv2','Monthly','vh_age','vh_value']\n",
    "booleanFeatures2=['order_pol_coverage']\n",
    "bnb.fit(X4,y1)\n",
    "\n",
    "print(bnb.score(X4,y1)) # accuracy - 0.6396170561189142\n",
    "preds = bnb.predict_proba()\n",
    "print(f1_score(y1,bnb.predict(X4))) #f1 - 0.21801284679513463 - GaussianNB\n",
    "\n",
    "confusion_matrix(y1,bnb.predict(X4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratic/Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(priors=[0.55,0.45],store_covariance=True)\n",
    "# lda = QuadraticDiscriminantAnalysis(priors=[0.75,0.25],store_covariance=True)\n",
    "lda.fit(X4,y1)\n",
    "print(lda.score(X4,y1))\n",
    "print(f1_score(y1,lda.predict(X4))) #f1 - 0.21100518481351396\n",
    "confusion_matrix(y1,lda.predict(X4))\n",
    "\n",
    "#0.22177432255174842 - lda = LinearDiscriminantAnalysis(priors=[0.55,0.45],store_covariance=True)\n",
    "\n",
    "#and 0.21121506055214373 - lda = QuadraticDiscriminantAnalysis(priors=[0.75,0.25],store_covariance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "X = df2[numericFeatures]\n",
    "X = ((X-X.mean())/X.std())\n",
    "pca = PCA()\n",
    "X2 = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor, RadiusNeighborsRegressor#NeighborhoodComponentsAnalysis, NearestCentroid\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# nca = NeighborhoodComponentsAnalysis(init='pca')\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y1,test_size=0.9)\n",
    "# nc = NearestCentroid(metric='euclidean')\n",
    "# nc.fit(X2,y1)\n",
    "# print(f1_score(y1,nc.predict(X2)))\n",
    "# confusion_matrix(y1,nc.predict(X2))\n",
    "\n",
    "# knn = KNeighborsClassifier(n_neighbors=3,weights='uniform',n_jobs=-1,p=1)\n",
    "# scores = cross_val_score(knn,X2,y1,scoring='f1')\n",
    "# print(scores.mean())\n",
    "knn.fit(X2,y1)\n",
    "# plt.scatter(X2[:,0],X2[:,1],c=df2['made_claim'],alpha=0.5)\n",
    "\n",
    "preds = knn.predict(X2)\n",
    "print(f1_score(y1,preds))\n",
    "confusion_matrix(y1,preds)\n",
    "\n",
    "# #0.3240575448913158, p=1 n_neighbors=3 f1_score on pcaed componentsd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC,NuSVC,OneClassSVM\n",
    "svc = LinearSVC(C=0.5,penalty='l1',max_iter=3000,dual=False,class_weight={0:1,1:8}) #0.21847693375787028\n",
    "# svc = NuSVC(nu=0.15,kernel='rbf',class_weight={0:1,1:1}) #0.19431386636559583\n",
    "# svc = OneClassSVM(kernel='linear',cache_size=500)\n",
    "svc.fit(X4,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = svc.predict(X4)\n",
    "print(r2_score(y1,preds))\n",
    "print(f1_score(y1,preds))\n",
    "print(classification_report(y1,preds))\n",
    "confusion_matrix(y1,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other -  SGD / Perceptron, Gaussian Mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import Perceptron, SGDClassifier\n",
    "# gpc = SGDClassifier(loss='log',penalty='l2',early_stopping=True,class_weight={0:1,1:9},max_iter=2000)\n",
    "# gpc.fit(X3,y1)\n",
    "# print(f1_score(y1,gpc.predict(X3)))\n",
    "# confusion_matrix(y1,gpc.predict(X3))\n",
    "# dict(sorted(zip(list(X3.columns)+['intercept'],list(gpc.coef_[0])+list(gpc.intercept_)),key=lambda k: abs(k[1]),reverse=True))\n",
    "\n",
    "\n",
    "# from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n",
    "\n",
    "# gm = BayesianGaussianMixture(n_components=2)\n",
    "# #f1 - 0.18451121455776556\n",
    "# gm.fit(df2[booleanFeatures2],y1)\n",
    "# print(f1_score(y1,gm.predict(df2[booleanFeatures2])))\n",
    "# confusion_matrix(y1,gm.predict(df2[booleanFeatures2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "1. Use Logistic Regression to predict probability, then use Linear Regression to predict severity including the probability\n",
    "    1. Use Linear Regression to predict severity, then use Logistic Regression to predict probability including predicted severity\n",
    "2. Same but Use Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Classifiers\n",
    "\n",
    "[x] Logistic Regression  - 0.2 F1 so far\n",
    "\n",
    "[x] Naive bayes - some success using ComplementNB\n",
    "\n",
    "[x] GradientBoosting \n",
    "\n",
    "[x] Random Forest, ExtraTrees - similar performance\n",
    "\n",
    "[x] XGBOOST\n",
    "\n",
    "[x] SVC \n",
    "\n",
    "[x] KNeighbors - worked on numericFeatures - too large for submission\n",
    "\n",
    "Gaussian Mixture - Data not gaussian\n",
    "\n",
    "Gaussian Process - too slow\n",
    "\n",
    "SGD, Perceptron - didnt really work\n",
    "\n",
    "HistGradientBoosting - did not work\n",
    "\n",
    "BaggingClassifier, Adaboost - slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Metrics\n",
    "Accuracy - fraction of correct predictions - causes to predict default 0\n",
    "\n",
    "Balanced Accuracy\n",
    "\n",
    "Precision - ability to not have false negatives\n",
    "Recall - Ability to not find true positives\n",
    "\n",
    "*F1 score* - weighted measure of precision and recall\n",
    "\n",
    "ROC curve -  performance of a binary classifier system as its discrimination threshold is varied\n",
    "\n",
    "roc_auc_curve score\n",
    "\n",
    "brier_score_loss\n",
    "\n",
    "hinge-loss\n",
    "\n",
    "roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "PCA\n",
    "\n",
    "Polynomial Features / Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
